<?php
return [
    "meta-title" => "Robots.txt Generator for SEO - Custom and Check robots.txt File in Free",
    "meta-desc" => "Use robots.txt generator to set the index status of the link on your website. Helps Google optimize dan improve your web performance.",
    "title" => "ROBOT.TXT GENERATOR",
    "subtitle" => "Generate robots.txt file easily",
    "label-robot-access" => "Default robot access",
    "placeholder-robot-access" => "Select Access",
    "robot-access-opt-1" => "Allow",
    "robot-access-opt-2" => "Disallow",
    "label-crawl-delay" => "Crawl delay",
    "placeholder-crawl-delay" => "Select delay",
    "crawl-delay-opt-1" => "No Delay",
    "crawl-delay-opt-2" => "5 seconds",
    "crawl-delay-opt-3" => "10 seconds",
    "crawl-delay-opt-4" => "20 seconds",
    "crawl-delay-opt-5" => "60 seconds",
    "crawl-delay-opt-6" => "120 seconds",
    "label-sitemap" => "Sitemap",
    "label-sitemap-helper" => "(leave blank if you donâ€™t have)",
    "label-directive" => "Directive",
    "btn-add" => "ADD DIRECTIVE",
    "highlight" => "In this latest version, we develop the Robot TXT Generator tool with export features and useragent features. The export feature will make it easier for you to check the code on Google Rich Result. Meanwhile, the useragent feature will allow you to add more commands to the Robot TXT Generator. This makes it easier for the txt Robot to specifically sort out which content you want to cover and which ones are displayed.",
    "desc-1" => "What Is Robot.txt Generator from cmlabs?",
    "desc-1-1" => "The robot exclusion standard, also called the robot exclusion protocol or nearly robot.txt, is a trend used by websites to talk to web crawlers and other web robots. Popular determines how to notify web robots about which areas of the internet site should not now be processed or scanned.",
    "desc-1-2" => "In practice, the robots.txt file is a protocol that decides whether a certain user agent (internet crawling software) is allowed or prohibited to crawl website elements. This crawl command is ensured by \"prohibiting\" or \"permitting\" positive (or all) behavior of consumer agents.",
    "desc-2" => "Why Does Your Website Need Robots.txt?",
    "desc-2-1" => "The Robots.Txt document manipulates crawlers to enter specific areas of your web page. While this can be very dangerous if you accidentally ban Googlebot from crawling your entire webpage (!!), there are a few situations where the robots.Txt document can be very useful.",
    "desc-2-2" => "If there isn't an area on your website that you need to control consumer agent access to, you now can't want a robots.txt record at all.",
    "desc-3" => "Robots.txt Main Functions For Your Website",
    "desc-3-1" => "Robots.txt is required if you want to restrict search engine bot access to some content on the website. Using Robots.txt you can set which content you want displayed on web pages.",
    "desc-3-2" => "Some content on the website may require restricted access rights. In this case, Robots.txt serves as a security to guide visitors because not all visitors have the same access to a website.",
    "desc-3-3" => "Robots.txt can enable the disallow feature on the folders you want to block so that Googlebot doesn't crawl the data. If the website doesn't need to block any files or data then the Robots.txt file is not needed. The use of Robots.txt is useful for maximizing the SEO function of the website.",
    "desc-3-4" => "More specifically, Robots.txt functions to share which content you want to display or close. In some cases, the content may not be appropriate or even interfere with the appearance of the website content. It is important to guide users to focus more on the core content and to capture information more quickly.",
    "desc-3-5" => "In conclusion, Robots.txt has functions to control the performance of spiderbots, limit the activity of bot robots, block content pages that you don't want to display, index website information, protect website data from being hacked or stolen by hackers, and to control Google or search engines to access the website.",
    "desc-4" => "The Location of The Robots.Txt File",
    "desc-4-1" => "In fact, the Robots.txt file is already in the root folder on the file storage server (public_html). Robots.txt is a virtual file that cannot be modified or accessed by other directories. You will not find a Robots.txt file in it when you open public_html. To be able to change or change the rules in robots.txt, you must first add a new file.",
    "desc-4-2" => "Create and place a new robots.txt file in the public_html folder and manually add the configuration script. This new file is used to replace files that will be overwritten in the previous configuration file.",
    "desc-5" => "Robots.txt Scenario / Tag Command",
    "desc-5-1" => "Robots.txt works according to the commands entered by the user. Commands are entered in the syntax according to the needs of the website. The following is an example of syntax:",
    "desc-5-1-1" => "Disallow: /admin/ = Is the syntax used to prohibit search engine bots from browsing or crawling the website admin folder",
    "desc-5-1-2" => "Disallow: /config/ = Is the syntax used to prohibit search engine bots from browsing or crawling the config folder on a website",
    "desc-5-1-3" => "User-agent: * = Is the syntax that is used to indicate that rules are created for all types of robots belonging to search engines",
    "desc-5-1-4" => "Allow: / = Is a syntax which indicates that the website allows robots to crawl or search folder data. This syntax is the reverse of the syntax disallowed.",
    "desc-5-2" => "Note that the allow and disallow syntax is customizable. Just add the specific folder name that you want to protect in the syntax.",
    "desc-6" => "How to Activate Robot.txt in WordPress",
    "desc-6-1" => "By using the All in One SEO Pack Plugin",
    "desc-6-1-1" => "There are 4 steps you have to take when using the All in One SEO Plugin, see:",
    "desc-6-1-1-1" => "First install the All in One SEO Package first",
    "desc-6-1-1-2" => "Features Manager",
    "desc-6-1-1-2-1" => "When finished installing All in One SEO, enter the menu, activate the robots.txt menu in the menu.",
    "desc-6-1-1-3" => "Click the Robots.txt section",
    "desc-6-1-1-3-1" => "If the robots.txt menu is active then you can see the robots.txt menu on the right hand panel. In this menu you can add user agents, rules and directory paths in a file.",
    "desc-6-1-1-4" => "Add rules",
    "desc-6-1-1-4-1" => "After all the steps are done, you can add rules in the plugin. In creating rules, you can customize the settings you want and can adapt them to search engines.",
    "desc-6-2" => "By manually uploading the Robots.txt file",
    "desc-6-2-1" => "Setting up robots.txt in WordPress can be set up manually, by using FTP or by accessing the hosting panel.",
    "desc-6-2-1-1" => "First of all, create a robots.txt file",
    "desc-6-2-1-1-1" => "Enter the rules in the file and then save.",
    "desc-6-2-1-2" => "Upload the robots.txt file that was created to the hosting server",
    "desc-6-2-1-2-1" => "To upload the robots.txt file you created earlier, you can use FTP. Apart from that, you can also host via the hosting admin panel.",
    "desc-6-3" => "By using the Yoast SEO Plugin",
    "desc-6-3-1" => "Install the plugin first",
    "desc-6-3-1-1" => "To get started, first install the Yoast SEO Plugin on your laptop or computer.",
    "desc-6-3-2" => "Go to the Tools page",
    "desc-6-3-2-1" => "After completing the installation, proceed to the next step, go to the menu section. Choose \"SEO Tools\". There are several menu options in this section that will appear by clicking 'File Editor.'",
    "desc-6-3-3" => "Create a Robots.txt file",
    "desc-6-3-3-1" => "The next step is \"Create a New Robots.txt\" in this section you can write down any rules you want to use and apply. Adjust it to your needs.",
    "whats-new-1" => "In this latest version, cmlabs added the export feature in the Robot TXT Generator tool. This feature is useful for users to directly check the code on Google Rich Result. The process of crawling websites and reviewing all content is now faster and more precise. You can learn more about this feature by trying it in person.",
    "whats-new-2" => "In this latest version update, cmlabs added more useragent features. This feature allows more commands that the TXT Generator Robot can receive. So, Robot.txt is more specific in sorting out which content you want to cover and which ones you want to display. Better update for better work.",
];
